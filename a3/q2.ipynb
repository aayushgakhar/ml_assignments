{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.data import loadlocal_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    activations = ['sigmoid', 'tanh', 'relu',\n",
    "                   'leaky_relu', 'linear', 'softmax']\n",
    "    weight_init_funcs = ['zero', 'random', 'normal']\n",
    "\n",
    "    def __init__(self, n_layers, layer_sizes, lr, activation, weight_init_func, epochs, batch_size) -> None:\n",
    "        self.n_layers = n_layers\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.lr = lr\n",
    "        self.activation = activation\n",
    "        self.weight_init_func = weight_init_func\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.w = {}\n",
    "        self.b = {}\n",
    "        self.init_weights()\n",
    "        \n",
    "    def activation_func(self, x, derivative=False):\n",
    "        activation = self.activation\n",
    "        if activation == 'sigmoid':\n",
    "            if derivative:\n",
    "                t = self.activation_func(x,activation)\n",
    "                return t * (1 - t)\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif activation == 'tanh':\n",
    "            if derivative:\n",
    "                t = self.activation_func(x, activation)\n",
    "                return 1 - t**2\n",
    "            return np.tanh(x)\n",
    "        elif activation == 'relu':\n",
    "            if derivative:\n",
    "                return np.where(x > 0, 1, 0)\n",
    "            return np.where(x > 0, x, 0)\n",
    "        elif activation == 'leaky_relu':\n",
    "            if derivative:\n",
    "                return np.where(x > 0, 1, 0.01)\n",
    "            return np.where(x > 0, x, 0.01 * x)\n",
    "        elif activation == 'linear':\n",
    "            if derivative:\n",
    "                return np.ones(x.shape)\n",
    "            return x\n",
    "        elif activation == 'softmax':\n",
    "            if derivative:\n",
    "                t = self.activation_func(x, activation)\n",
    "                return t * (1 - t)\n",
    "            return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        \n",
    "    def _weight(self, shape, weight_init_func):\n",
    "        if weight_init_func == 'zero':\n",
    "            return np.zeros(shape)\n",
    "        elif weight_init_func == 'random':\n",
    "            return np.random.rand(shape[0],shape[1])*0.01\n",
    "        elif weight_init_func == 'normal':\n",
    "            return np.random.randn(shape[0],shape[1])*0.01\n",
    "        \n",
    "    \n",
    "    def init_weights(self):\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.w[i] = self._weight((self.layer_sizes[i-1], self.layer_sizes[i]), self.weight_init_func)\n",
    "            self.b[i] = self._weight((1, self.layer_sizes[i]), self.weight_init_func)\n",
    "        return self.w, self.b\n",
    "    \n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        for i in range(1, self.n_layers):\n",
    "            \n",
    "            X = self.activation_func(X, self.activation)\n",
    "        caches = {}\n",
    "        A = X\n",
    "\n",
    "        # print(X.shape)\n",
    "\n",
    "        L = self.n_layers\n",
    "\n",
    "        for i in range(0, L-1):\n",
    "\n",
    "            A_prev = A\n",
    "            Z = np.dot(A_prev, self.w[i+1]) + self.b[i+1]\n",
    "\n",
    "            A = self.activation_func(Z)\n",
    "\n",
    "            caches[str(i+1)] = (Z, A)\n",
    "            A_prev = A\n",
    "\n",
    "        Z_l = np.dot(A_prev, self.w[L]) + self.b[L]\n",
    "        A_l = self.activation_func(Z_l)\n",
    "\n",
    "        caches[str(L)] = (Z_l, A_l)\n",
    "\n",
    "        return A_l, caches\n",
    "            \n",
    "    \n",
    "    def backward(self, X, y, caches):\n",
    "        grads = {}\n",
    "\n",
    "        L = self.n_layers - 1\n",
    "        Lx = len(X)\n",
    "        caches[str(0)] = (X, X)\n",
    "\n",
    "        A = caches[str(L)][1]\n",
    "\n",
    "        dZ = A-y\n",
    "\n",
    "        dW = np.dot(caches[str(L-1)][1].T, dZ) / Lx\n",
    "        db = np.sum(dZ, axis=0, keepdims=True) / Lx\n",
    "\n",
    "        dA_prev = np.dot(dZ, self.parameters[\"W\" + str(L)].T)\n",
    "\n",
    "        grads[\"dW\" + str(L)] = dW\n",
    "        grads[\"db\" + str(L)] = db\n",
    "\n",
    "        for i in range(L - 1, 0, -1):\n",
    "            d_act = self.activation_func(caches[str(i)][0], derivative=True)\n",
    "\n",
    "            # print(dA_prev.shape)\n",
    "            # print(d_act.shape)\n",
    "            # dZ = np.matmul(dA_prev, d_act)\n",
    "            dZ = dA_prev * d_act\n",
    "            # print(dZ)\n",
    "            dW = np.dot(caches[str(i-1)][1].T, dZ) / Lx\n",
    "            db = np.sum(dZ, axis=0, keepdims=True) / Lx\n",
    "\n",
    "            if i > 1:\n",
    "                dA_prev = np.dot(dZ, self.parameters[\"W\" + str(i)].T)\n",
    "\n",
    "            grads[\"dW\" + str(i)] = dW\n",
    "            grads[\"db\" + str(i)] = db\n",
    "\n",
    "        self.update_weights(grads)\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def update_weights(self, grads):\n",
    "        N = self.n_layers\n",
    "        \n",
    "        for i in range(1, N):\n",
    "            self.w[i] -= self.lr * grads['dW'+str(i)]\n",
    "            self.b[i] -= self.lr * grads['db'+str(i)]\n",
    "    \n",
    "    def labels_to_class(self, y):\n",
    "        m = len(y)\n",
    "        c = int(np.max(y))\n",
    "        y_classes = np.zeros((m, c+1))\n",
    "        for i in range(m):\n",
    "            l = int(y[i])\n",
    "            y_classes[i, l] = 1\n",
    "\n",
    "        return y_classes\n",
    "    \n",
    "    def reset_gradients(self):\n",
    "        pass\n",
    "    \n",
    "        \n",
    "    def fittt(self, X, y , x_test=None, y_test=None):\n",
    "        y = self.labels_to_class(y)\n",
    "        \n",
    "        m , n_0 = X.shape\n",
    "        n_l = y.shape[1]\n",
    "\n",
    "        parameters = self.initializeWeights()\n",
    "        self.parameters = parameters\n",
    "\n",
    "        train_loss_history = []\n",
    "        train_accuracy_history = []\n",
    "        test_loss_history = []\n",
    "        test_accuracy_history = []\n",
    "\n",
    "        \n",
    "\n",
    "        for epoch in tqdm(range(self.epochs), desc = \"Progress Total : \", position = 0, leave = True):\n",
    "\n",
    "\n",
    "            n_batches = m//self.batch_size\n",
    "            X_batches = [X[self.batch_size*i:self.batch_size*(i+1),:] for i in range(0,n_batches)]\n",
    "            y_batches = [y[self.batch_size*i:self.batch_size*(i+1),:] for i in range(0,n_batches)]\n",
    "\n",
    "            train_batch_loss = []\n",
    "            test_batch_loss = []\n",
    "            train_batch_accuracy = []\n",
    "            test_batch_accuracy = []\n",
    "\n",
    "            for curr_x, curr_y in tqdm(zip(X_batches,y_batches), desc = \"Progress Epoch: \" + str(epoch+1) + \"/\" + str(self.num_epochs), position = 0, leave = True, total = len(X_batches)):\n",
    "                A, caches = self.forward_propogation(curr_x,parameters)\n",
    "\n",
    "                train_cost = self.cross_entropy_loss(A,curr_y)\n",
    "                train_batch_loss.append(train_cost)\n",
    "#                 print(A)\n",
    "                self.backward_propogation(curr_x,curr_y, caches)\n",
    "#                 train_batch_accuracy.append(self.score(curr_x,np.argmax(curr_y,axis = 1)))\n",
    "                if(x_test is not None):\n",
    "                    proba = self.predict_proba(x_test)\n",
    "#                     print(proba.shape)\n",
    "                    test_loss = self.cross_entropy_loss(proba, self.labels_to_class(y_test))\n",
    "                    test_batch_loss.append(test_loss)\n",
    "#                     test_batch_accuracy.append(self.score(x_test, y_test))\n",
    "                    \n",
    "#             print(\"Training Accuracy : \", np.array(train_batch_accuracy).mean())\n",
    "#             print(\"Validation Accuracy : \", np.array(test_batch_accuracy).mean())\n",
    "            print(\"Testing loss : \" ,np.array(test_batch_loss).mean())\n",
    "            print(\"Training Loss : \", np.array(train_batch_loss).mean())\n",
    "            \n",
    "\n",
    "\n",
    "            train_loss_history.append( np.array(train_batch_loss).mean())\n",
    "#             train_accuracy_history.append( np.array(train_batch_accuracy).mean())\n",
    "            test_loss_history.append( np.array(test_batch_loss).mean())\n",
    "#             test_accuracy_history.append(  np.array(test_batch_accuracy).mean())\n",
    "                \n",
    "                \n",
    "        \n",
    "        self.train_loss_history = train_loss_history\n",
    "        self.train_accuracy_history = train_accuracy_history\n",
    "        self.test_loss_history = test_loss_history\n",
    "        self.test_accuracy_history = test_accuracy_history\n",
    "        \n",
    "        \n",
    "        self.parameters = parameters\n",
    "\n",
    "\n",
    "        return self  \n",
    "        \n",
    "        \n",
    "    def fit (self, X_train, Y_train, X_val, Y_val):\n",
    "        \n",
    "        Y_train = self.labels_to_class(Y_train)\n",
    "        \n",
    "        m = X_train.shape[0]\n",
    "        no_batches = m//self.batch_size\n",
    "        datas=[]\n",
    "        for subset in range(0,no_batches):\n",
    "            mini_X = X_train[self.batch_size*subset : self.batch_size*(subset+1), :]\n",
    "            mini_Y = Y_train[self.batch_size*subset : self.batch_size*(subset+1), :]\n",
    "            datas.append((mini_X,mini_Y))\n",
    "        \n",
    "        \n",
    "        print(X_train.shape)\n",
    "        print(Y_train.shape)\n",
    "        # print(X_test.shape)\n",
    "        # print(Y_test.shape)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        # self.parameters = params\n",
    "        \n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "      \n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            print(\"Epoch: \", epoch)\n",
    "            trainbatchloss = []\n",
    "            valbatchloss = []\n",
    "\n",
    "            for x_b, y_b in datas:\n",
    "                A_l, caches = self.forward(x_b)\n",
    "                train_cost = self.cross_entropy_loss(A_l, y_b)\n",
    "                trainbatchloss.append(train_cost)\n",
    "                self.backward(x_b,y_b, caches)\n",
    "                proba = self.predict_proba(X_val)\n",
    "                valloss = self.cross_entropy_loss(proba, self.labels_to_class(Y_val))\n",
    "                valbatchloss.append(valloss)\n",
    "\n",
    "            \n",
    "            l1 = np.array(trainbatchloss).mean()\n",
    "            l2 = np.array(valbatchloss).mean()\n",
    "\n",
    "\n",
    "            train_loss.append(l1)\n",
    "            val_loss.append(l2)\n",
    "            \n",
    "            print(\"Training loss : \" ,l1)\n",
    "            print(\"Validation Loss : \", l2)\n",
    "              \n",
    "              \n",
    "      \n",
    "        self.train_loss = train_loss\n",
    "        self.val_loss = val_loss\n",
    "  \n",
    "    \n",
    "     \n",
    "    def cross_entropy_loss(self, A_l, y_test):\n",
    "        \n",
    "        temp=A_l[np.arange(len(y_test)), y_test.argmax(axis=1)]\n",
    "        temp=np.where(temp>0.0000000000001,temp,0.000000000001)\n",
    "        logp = - np.log(temp)\n",
    "        celoss = np.sum(logp)/len(y_test)\n",
    "        return celoss\n",
    "        \n",
    "        \n",
    "        # m = len(y_test)\n",
    "        \n",
    "        # # logprods = np.dot(y_test, np.log(A_l).T) + np.dot((1-y_test), np.log(1-A_l).T)\n",
    "        # # cost = -1/n*np.sum(logprods)\n",
    "        \n",
    "        # logp = - np.log(1e-7 + A_l[np.arange(m), y_test.argmax(axis=1)])\n",
    "        # loss = np.sum(logp)/m\n",
    "        # return loss \n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        prob, caches = self.forward_propogation(X, self.parameters)\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        prob = self.predict_proba(X)\n",
    "        Y_prediction = np.argmax(prob, axis = 1)\n",
    "        return Y_prediction\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        Y_prediction = self.predict(X)\n",
    "        count = 0\n",
    "        for i in range(len(Y)):    \n",
    "            if (Y_prediction[i] == Y[i]):\n",
    "                count+=1\n",
    "        return count/len(Y)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # def fit(self,X, y):\n",
    "    #     for e in self.epochs:\n",
    "    #         print(f'Epoch {e}')\n",
    "    #         loss = 0\n",
    "            \n",
    "    #         for i in range(0, X.shape[0], self.batch_size):\n",
    "    #             X_batch = X[i:i+self.batch_size]\n",
    "    #             y_batch = y[i:i+self.batch_size]\n",
    "    #             loss += self.backward(X_batch, y_batch)\n",
    "    #             self.update_weights()\n",
    "    #             self.reset_gradients()\n",
    "    \n",
    "    # def predict(self,X):\n",
    "    #     pass\n",
    "\n",
    "    # def predict_proba(self,X):\n",
    "    #     pass\n",
    "\n",
    "    # def score(self,X, y):\n",
    "    #     pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = loadlocal_mnist(\n",
    "    images_path='../data/mnist/train-images-idx3-ubyte', labels_path='../data/mnist/train-labels-idx1-ubyte')\n",
    "test_images, test_labels = loadlocal_mnist(\n",
    "    images_path='../data/mnist/t10k-images-idx3-ubyte', labels_path='../data/mnist/t10k-labels-idx1-ubyte')\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.15)\n",
    "\n",
    "\n",
    "X_train, y_train = train_images, train_labels\n",
    "X_test, y_test = test_images, test_labels\n",
    "X_val, y_val = val_images, val_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51000, 784)\n",
      "(51000, 10)\n",
      "Epoch:  1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mlp \u001b[39m=\u001b[39m NeuralNetwork(\u001b[39m6\u001b[39m, [\u001b[39m784\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m10\u001b[39m], \u001b[39m0.1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mReLU\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnormal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m30\u001b[39m, \u001b[39m128\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m mlp\u001b[39m.\u001b[39;49mfit(X_train, y_train, X_val, y_val)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m{\u001b[39;00mmlp\u001b[39m.\u001b[39mscore(X_test, y_test)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [27], line 255\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, X_train, Y_train, X_val, Y_val)\u001b[0m\n\u001b[1;32m    252\u001b[0m valbatchloss \u001b[39m=\u001b[39m []\n\u001b[1;32m    254\u001b[0m \u001b[39mfor\u001b[39;00m x_b, y_b \u001b[39min\u001b[39;00m datas:\n\u001b[0;32m--> 255\u001b[0m     A_l, caches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x_b)\n\u001b[1;32m    256\u001b[0m     train_cost \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_entropy_loss(A_l, y_b)\n\u001b[1;32m    257\u001b[0m     trainbatchloss\u001b[39m.\u001b[39mappend(train_cost)\n",
      "Cell \u001b[0;32mIn [27], line 79\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, L\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     78\u001b[0m     A_prev \u001b[39m=\u001b[39m A\n\u001b[0;32m---> 79\u001b[0m     Z \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(A_prev, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw[i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     81\u001b[0m     A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_func(Z)\n\u001b[1;32m     83\u001b[0m     caches[\u001b[39mstr\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)] \u001b[39m=\u001b[39m (Z, A)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "mlp = NeuralNetwork(6, [784, 256, 128, 64, 32, 10], 0.1, 'ReLU', 'normal', 30, 128)\n",
    "mlp.fit(X_train, y_train, X_val, y_val)\n",
    "print(f'Accuracy: {mlp.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9668fcc41e1ce838214f10c723fd65249829dd257fba636b92eab1c9c805b2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
